package main

import (
	"context"
	"encoding/json"
	"os"
	"os/signal"
	"path/filepath"
	"runtime"
	"runtime/debug"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/rs/zerolog"
	"github.com/rs/zerolog/log"

	"dark-deep-new-tool/pkg/config"
	"dark-deep-new-tool/pkg/elastic"
	"dark-deep-new-tool/pkg/models"
	"dark-deep-new-tool/pkg/scraper"
	"dark-deep-new-tool/pkg/tor"
)

const (
	maxInMemoryRecords = 100
	cleanupInterval    = 15 * time.Minute
	maxConcurrent      = 3
	batchSize          = 10
	batchWaitTime      = 80 * time.Second   // â† Batch arasÄ± bekleme yok
	scrapeInterval     = 60 * time.Second  // â† DÃ¶ngÃ¼ bitince 10 sn sonra tekrar
	shutdownTimeout    = 30 * time.Second
)

var (
	forumData       []models.Forum
	forumCounter    int64
	contentChecker  *models.ContentChecker
	elasticClient   *elastic.ElasticClient
	dataFile        = "output/data.json"
	lastContentFile = "output/last_contents.json"
	failureDir      = "output/failure"
	dataMutex       sync.RWMutex
	isShuttingDown  int32
)

func main() {
	cwd, err := os.Getwd()
	if err != nil {
		log.Fatal().Err(err).Msg("Ã‡alÄ±ÅŸma dizini alÄ±namadÄ±")
	}

	// Dizinleri oluÅŸtur
	for _, dir := range []string{failureDir, "output"} {
		fullPath := filepath.Join(cwd, dir)
		if err := os.MkdirAll(fullPath, 0755); err != nil {
			log.Fatal().Err(err).Str("klasÃ¶r", fullPath).Msg("KlasÃ¶r oluÅŸturulamadÄ±")
		}
	}

	// Loglama ayarÄ±
	zerolog.TimeFieldFormat = zerolog.TimeFormatUnix
	logDir := filepath.Join(cwd, "logs")
	if err := os.MkdirAll(logDir, 0755); err != nil {
		log.Fatal().Err(err).Msg("Log klasÃ¶rÃ¼ oluÅŸturulamadÄ±")
	}
	logFilePath := filepath.Join(logDir, "app.log")
	logFile, err := os.OpenFile(logFilePath, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		log.Fatal().Err(err).Msg("Log dosyasÄ± aÃ§Ä±lamadÄ±")
	}
	defer logFile.Close()

	multi := zerolog.MultiLevelWriter(os.Stdout, logFile)
	log.Logger = zerolog.New(multi).With().Timestamp().Logger()

	log.Info().Msg("ğŸš€ Program baÅŸlatÄ±ldÄ±")

	// Content checker baÅŸlat
	contentChecker = models.NewContentChecker(filepath.Join(cwd, lastContentFile))
	log.Info().Msg("ğŸ“‹ Ä°Ã§erik karÅŸÄ±laÅŸtÄ±rma sistemi baÅŸlatÄ±ldÄ± (SHA-256 Hash)")

	// Son iÃ§erikleri yÃ¼kle
	if err := contentChecker.LoadFromFile(); err != nil {
		log.Warn().Err(err).Msg("Son iÃ§erikler yÃ¼klenemedi, sÄ±fÄ±rdan baÅŸlanÄ±yor")
	} else {
		log.Info().
			Int("yÃ¼klenen_url", contentChecker.Count()).
			Msg("ğŸ“š Son iÃ§erikler yÃ¼klendi")
	}

	// KonfigÃ¼rasyon yÃ¼kle
	configPath := filepath.Join(cwd, "config.yaml")
	cfg, err := config.LoadConfig(configPath)
	if err != nil {
		log.Fatal().Err(err).Msg("Config yÃ¼klenemedi")
	}
	log.Info().Int("forum_sayÄ±sÄ±", len(cfg.Forums)).Msg("âš™ï¸ KonfigÃ¼rasyon yÃ¼klendi")

	// Tor istemcisi
	torClient, err := tor.NewTorClient()
	if err != nil {
		log.Fatal().Err(err).Msg("Tor client baÅŸlatÄ±lamadÄ±")
	}
	log.Info().Msg("ğŸ§… Tor client hazÄ±r")

	// Elasticsearch client
	elasticCfg, err := config.LoadElasticConfig()
	if err != nil {
		log.Fatal().Err(err).Msg("Elasticsearch config yÃ¼klenemedi")
	}

	elasticClient, err = elastic.NewElasticClient(
		elasticCfg.URL,
		elasticCfg.Username,
		elasticCfg.Password,
		elasticCfg.Index,
		elasticCfg.SkipVerify,
		elasticCfg.MaxRetries,
		elasticCfg.RetryBackoff,
	)
	if err != nil {
		log.Fatal().Err(err).Msg("Elasticsearch client baÅŸlatÄ±lamadÄ±")
	}

	// Elasticsearch baÄŸlantÄ±sÄ±nÄ± test et
	if err := elasticClient.TestConnection(); err != nil {
		log.Fatal().Err(err).Msg("Elasticsearch baÄŸlantÄ± testi baÅŸarÄ±sÄ±z")
	}
	log.Info().
		Str("url", elasticCfg.URL).
		Str("index", elasticCfg.Index).
		Msg("ğŸ“Š Elasticsearch client hazÄ±r")

	// Scraper oluÅŸtur
	s := scraper.NewScraperWithBrowsers(torClient)
	log.Info().Msg("ğŸŒ 2 Chrome browser hazÄ±r")
	log.Info().Msg("ğŸ’¾ RAM optimize: ~400MB (2 browser)")

	// JSON'dan eski verileri yÃ¼kle
	loadExistingData(cwd)

	// Ä°lk memory durumu
	logMemoryStats("BaÅŸlangÄ±Ã§")

	// Ana context ve cancel fonksiyonu
	mainCtx, mainCancel := context.WithCancel(context.Background())
	defer mainCancel()

	// WaitGroup for goroutines
	var wg sync.WaitGroup

	// Signal handler
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	// Cleanup ticker
	cleanupTicker := time.NewTicker(cleanupInterval)
	defer cleanupTicker.Stop()

	// Scrape ticker
	scrapeTicker := time.NewTicker(scrapeInterval)
	defer scrapeTicker.Stop()

	// Ä°lk tarama hemen
	log.Info().Msg("ğŸ” Ä°lk tarama dÃ¶ngÃ¼sÃ¼ baÅŸlatÄ±lÄ±yor...")
	if err := scrapeCycleWithBatch(mainCtx, s, cfg, cwd); err != nil {
		if err != context.Canceled {
			log.Error().Err(err).Msg("Ä°lk tarama dÃ¶ngÃ¼sÃ¼ baÅŸarÄ±sÄ±z")
		}
	}
	saveToJSON(cwd)
	saveContentChecker()
	performCleanup()
	log.Info().Msg("âœ… Ä°lk tarama dÃ¶ngÃ¼sÃ¼ tamamlandÄ±")

	// Ana dÃ¶ngÃ¼
	wg.Add(1)
	go func() {
		defer wg.Done()

		for {
			select {
			case <-mainCtx.Done():
				log.Info().Msg("ğŸ›‘ Ana dÃ¶ngÃ¼ sonlandÄ±rÄ±lÄ±yor...")
				return

			case <-sigChan:
				log.Info().Msg("ğŸ›‘ Kapatma sinyali alÄ±ndÄ± (CTRL+C)")
				atomic.StoreInt32(&isShuttingDown, 1)

				// Graceful shutdown with timeout
				shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), shutdownTimeout)
				defer shutdownCancel()

				shutdownChan := make(chan struct{})

				go func() {
					log.Info().Msg("ğŸ’¾ Veriler kaydediliyor...")
					saveToJSON(cwd)
					saveContentChecker()
					performCleanup()

					log.Info().Msg("ğŸ§¹ Browser'lar kapatÄ±lÄ±yor...")
					s.Close()

					logMemoryStats("KapanÄ±ÅŸ")
					close(shutdownChan)
				}()

				select {
				case <-shutdownChan:
					log.Info().Msg("ğŸ‘‹ Program gÃ¼venli ÅŸekilde kapatÄ±ldÄ±")
				case <-shutdownCtx.Done():
					log.Warn().Msg("âš ï¸ Graceful shutdown timeout, zorla kapatÄ±lÄ±yor")
				}

				mainCancel()
				return

			case <-cleanupTicker.C:
				if atomic.LoadInt32(&isShuttingDown) == 1 {
					return
				}
				log.Info().Msg("ğŸ§¹ Periyodik cleanup baÅŸlatÄ±lÄ±yor...")
				performCleanup()
				saveContentChecker()
				logMemoryStats("Cleanup SonrasÄ±")

			case <-scrapeTicker.C:
				if atomic.LoadInt32(&isShuttingDown) == 1 {
					return
				}
				log.Info().Msg("â° Yeni tarama dÃ¶ngÃ¼sÃ¼ baÅŸlatÄ±lÄ±yor...")
				if err := scrapeCycleWithBatch(mainCtx, s, cfg, cwd); err != nil {
					if err != context.Canceled {
						log.Error().Err(err).Msg("Tarama dÃ¶ngÃ¼sÃ¼ baÅŸarÄ±sÄ±z")
					}
				}
				saveToJSON(cwd)
				saveContentChecker()
				log.Info().Msg("âœ… Tarama dÃ¶ngÃ¼sÃ¼ tamamlandÄ±")
			}
		}
	}()

	// Wait for graceful shutdown
	wg.Wait()

	// Final cleanup
	log.Info().Msg("ğŸ Program tamamen sonlandÄ±rÄ±ldÄ±")
	os.Exit(0)
}

// scrapeCycleWithBatch siteleri batch'ler halinde tarar
func scrapeCycleWithBatch(ctx context.Context, s *scraper.Scraper, cfg config.Config, cwd string) error {
	totalForums := len(cfg.Forums)

	for i := 0; i < totalForums; i += batchSize {
		// Shutdown kontrolÃ¼
		if atomic.LoadInt32(&isShuttingDown) == 1 {
			return context.Canceled
		}

		// Context kontrolÃ¼
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		end := i + batchSize
		if end > totalForums {
			end = totalForums
		}

		batch := cfg.Forums[i:end]
		batchNum := (i / batchSize) + 1
		totalBatches := (totalForums + batchSize - 1) / batchSize

		log.Info().
			Int("batch", batchNum).
			Int("toplam_batch", totalBatches).
			Int("site_sayÄ±sÄ±", len(batch)).
			Msg("ğŸ“¦ Batch tarama baÅŸlatÄ±lÄ±yor")

		if err := scrapeBatch(ctx, s, batch, cwd); err != nil {
			if err == context.Canceled {
				return err
			}
			log.Error().Err(err).Int("batch", batchNum).Msg("Batch tarama baÅŸarÄ±sÄ±z")
		}

		// Batch'ler arasÄ± bekleme (son batch hariÃ§)
		if end < totalForums {
			log.Info().
				Dur("bekleme", batchWaitTime).
				Msg("â¸ï¸ Sonraki batch iÃ§in bekleniyor")

			select {
			case <-ctx.Done():
				return ctx.Err()
			case <-time.After(batchWaitTime):
				// Continue
			}
		}
	}

	return nil
}

// scrapeBatch bir batch'teki siteleri tarar
func scrapeBatch(ctx context.Context, s *scraper.Scraper, batch []models.ForumEntry, cwd string) error {
	sem := make(chan struct{}, maxConcurrent)
	var wg sync.WaitGroup

	for _, entry := range batch {
		// Shutdown kontrolÃ¼
		if atomic.LoadInt32(&isShuttingDown) == 1 {
			break
		}

		wg.Add(1)
		go func(e models.ForumEntry) {
			defer wg.Done()

			// Context kontrolÃ¼
			select {
			case <-ctx.Done():
				return
			case sem <- struct{}{}:
				defer func() { <-sem }()
			}

			log.Info().Str("forum", e.Name).Msg("ğŸ“¡ Tarama baÅŸlatÄ±lÄ±yor")

			// Scraper'dan veriyi al
			scraperData, err := s.Scrape(e, cwd)
			if err != nil {
				log.Error().Err(err).Str("forum", e.Name).Msg("âŒ Tarama baÅŸarÄ±sÄ±z")
				return
			}

			// BoÅŸ content kontrolÃ¼
			if scraperData.Content == "" {
				log.Info().Str("forum", e.Name).Msg("âš ï¸ Ä°Ã§erik boÅŸ, atlanÄ±yor")
				return
			}

			// Ä°Ã§erik karÅŸÄ±laÅŸtÄ±rma (SHA-256 Hash)
			if contentChecker.IsDuplicate(scraperData.Source, scraperData.Link, scraperData.Content) {
				log.Info().
					Str("forum", e.Name).
					Str("url", e.URL).
					Str("link", scraperData.Link).
					Str("Ã¶nizleme", truncateString(scraperData.Content, 40)).
					Msg("ğŸ”„ Ä°Ã§erik deÄŸiÅŸmemiÅŸ, atlanÄ±yor")
				return
			}

			// YENÄ° Ä°Ã‡ERÄ°K TESPÄ°T EDÄ°LDÄ°
			log.Info().
				Str("forum", e.Name).
				Str("url", e.URL).
				Str("link", scraperData.Link).
				Msg("ğŸ†• YENÄ° iÃ§erik tespit edildi")

			// Elasticsearch uyumlu Forum struct'Ä± oluÅŸtur
			data := models.NewForum(
				scraperData.Source,
				scraperData.Content,
				scraperData.Author,
				scraperData.Link,
				e.Type,
			)

			// Content hash'i hesapla
			contentHash := contentChecker.Update(data.Source, data.Link, data.Content)
			data.ContentHash = contentHash

			// Elasticsearch'e ANINDA gÃ¶nder
			if err := saveToElastic(ctx, data); err != nil {
				log.Warn().
					Err(err).
					Str("forum", e.Name).
					Msg("âš ï¸ Elasticsearch'e gÃ¶nderilemedi ama devam ediliyor")
			}

			dataMutex.Lock()
			defer dataMutex.Unlock()

			// Yeni veri ekle
			atomic.AddInt64(&forumCounter, 1)
			data.ID = int(atomic.LoadInt64(&forumCounter))

			// Bellekte sadece son 100 kayÄ±t tut
			if len(forumData) >= maxInMemoryRecords {
				forumData = forumData[1:]
			}
			forumData = append(forumData, data)

			log.Info().
				Str("forum", e.Name).
				Str("link", data.Link).
				Str("hash", contentHash[:16]+"...").
				Str("type", data.Type).
				Int("kayÄ±t_no", data.ID).
				Int("bellekteki_kayÄ±t", len(forumData)).
				Msg("âœ… YENÄ° VERÄ° kaydedildi")
		}(entry)
	}

	// Goroutine'lerin bitmesini bekle
	done := make(chan struct{})
	go func() {
		wg.Wait()
		close(done)
	}()

	// Context cancel veya goroutines complete
	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-done:
		return nil
	}
}

// loadExistingData JSON'dan eski verileri yÃ¼kler
func loadExistingData(cwd string) {
	dataPath := filepath.Join(cwd, dataFile)

	if _, err := os.Stat(dataPath); err != nil {
		log.Info().Msg("ğŸ“‚ Eski veri bulunamadÄ±, sÄ±fÄ±rdan baÅŸlanÄ±yor")
		return
	}

	data, err := os.ReadFile(dataPath)
	if err != nil {
		log.Warn().Err(err).Msg("Eski veri okunamadÄ±")
		return
	}

	dataMutex.Lock()
	defer dataMutex.Unlock()

	if err := json.Unmarshal(data, &forumData); err != nil {
		log.Warn().Err(err).Msg("Eski veri parse edilemedi")
		return
	}

	// Sadece son 100 kayÄ±t tut
	if len(forumData) > maxInMemoryRecords {
		forumData = forumData[len(forumData)-maxInMemoryRecords:]
	}

	atomic.StoreInt64(&forumCounter, int64(len(forumData)))
	log.Info().
		Int("yÃ¼klenen_kayÄ±t", len(forumData)).
		Int("url_count", contentChecker.Count()).
		Msg("ğŸ“š Eski veriler baÅŸarÄ±yla yÃ¼klendi")
}

// saveToJSON tÃ¼m veriyi JSON'a kaydeder
func saveToJSON(cwd string) {
	dataMutex.RLock()
	defer dataMutex.RUnlock()

	if len(forumData) == 0 {
		log.Info().Msg("ğŸ’¾ Kaydedilecek veri yok")
		return
	}

	data, err := json.MarshalIndent(forumData, "", "  ")
	if err != nil {
		log.Error().Err(err).Msg("JSON encode hatasÄ±")
		return
	}

	dataPath := filepath.Join(cwd, dataFile)

	if err := os.MkdirAll(filepath.Dir(dataPath), 0755); err != nil {
		log.Error().Err(err).Msg("Data klasÃ¶rÃ¼ oluÅŸturulamadÄ±")
		return
	}

	if err := os.WriteFile(dataPath, data, 0644); err != nil {
		log.Error().Err(err).Msg("JSON yazma hatasÄ±")
	} else {
		log.Info().
			Int("toplam_kayÄ±t", len(forumData)).
			Str("dosya", dataPath).
			Msg("ğŸ’¾ Veriler baÅŸarÄ±yla kaydedildi")
	}
}

// saveContentChecker son iÃ§erikleri kaydeder
func saveContentChecker() {
	if err := contentChecker.SaveToFile(); err != nil {
		log.Error().Err(err).Msg("Son iÃ§erikler kaydedilemedi")
	} else {
		log.Info().Int("url_count", contentChecker.Count()).Msg("ğŸ’¾ Son iÃ§erikler kaydedildi")
	}
}

// saveToElastic Elasticsearch'e tek kayÄ±t gÃ¶nderir
func saveToElastic(ctx context.Context, data models.Forum) error {
	if elasticClient == nil {
		log.Warn().Msg("âš ï¸ Elasticsearch client yok, atlÄ±yor")
		return nil
	}

	// Elasticsearch'e ANINDA gÃ¶nder
	if err := elasticClient.IndexDocument(ctx, data); err != nil {
		log.Error().
			Err(err).
			Str("source", data.Source).
			Str("link", data.Link).
			Msg("âŒ Elasticsearch'e gÃ¶nderilemedi")
		return err
	}

	log.Info().
		Str("source", data.Source).
		Str("type", data.Type).
		Str("hash", data.ContentHash[:16]+"...").
		Msg("ğŸ“Š Elasticsearch'e baÅŸarÄ±yla gÃ¶nderildi")

	return nil
}

// performCleanup memory cleanup yapar
func performCleanup() {
	dataMutex.Lock()
	defer dataMutex.Unlock()

	initialCount := len(forumData)

	// Bellekte max 100 kayÄ±t tut
	if len(forumData) > maxInMemoryRecords {
		removedCount := len(forumData) - maxInMemoryRecords
		forumData = forumData[removedCount:]

		log.Info().
			Int("silinen_kayÄ±t", removedCount).
			Int("kalan_kayÄ±t", len(forumData)).
			Msg("âœ‚ï¸ Eski kayÄ±tlar bellekten temizlendi")
	}

	log.Info().
		Int("Ã¶nceki_kayÄ±t", initialCount).
		Int("ÅŸimdiki_kayÄ±t", len(forumData)).
		Msg("ğŸ§¹ Cleanup tamamlandÄ±")

	// Garbage collection
	runtime.GC()
	debug.FreeOSMemory()

	log.Info().Msg("â™»ï¸ Garbage collection Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±")
}

// logMemoryStats memory istatistiklerini loglar
func logMemoryStats(phase string) {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)

	log.Info().
		Str("phase", phase).
		Uint64("alloc_mb", m.Alloc/1024/1024).
		Uint64("total_alloc_mb", m.TotalAlloc/1024/1024).
		Uint64("sys_mb", m.Sys/1024/1024).
		Uint32("num_gc", m.NumGC).
		Int("forum_data_count", len(forumData)).
		Int("url_count", contentChecker.Count()).
		Msg("ğŸ“Š Memory Stats")
}

// truncateString string'i keser
func truncateString(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}
